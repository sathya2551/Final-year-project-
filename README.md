# Sri Krishna College Chatbot — Complete Setup & Demo

This repository contains a simple intent-based chatbot for Sri Krishna College of Arts and Science (Coimbatore). This README is a complete, step-by-step guide so a new developer can reproduce the project from scratch on Windows, WSL, or GitHub Codespaces. Copy–paste the commands shown exactly.

## Files in repository
- `intents.json` — chatbot dataset (dummy college data; replace placeholders before final submission)
- `train.py` — training script (TF‑IDF + lemmatization)
- `app.py` — Flask server that loads the model and serves the chat API
- `templates/index.html` — web UI
- `static/style.css` — web UI styles
- `model.pkl`, `vectorizer.pkl` — trained artifacts (generated by `train.py`)
- `chat_logs.txt` — conversation log (generated at runtime)

## Prerequisites
- Python 3.8+ (download): https://www.python.org/downloads/
- Visual Studio Code (recommended): https://code.visualstudio.com/
- Git: https://git-scm.com/downloads
- (Optional) WSL2 on Windows: https://learn.microsoft.com/windows/wsl/install

## Quick overview (what you'll do)
1. Create project folder and files (or clone this repo).
2. Create and activate a virtual environment.
3. Install dependencies.
4. Train the model with `train.py` to produce `model.pkl` and `vectorizer.pkl`.
5. Run the Flask app `app.py` and open the web UI.

---

## A. Windows PowerShell — Full copy/paste setup
Open PowerShell and run these commands (adjust paths if needed):

```powershell
# 1) Create project folder (if starting fresh)
mkdir C:\Users\$env:USERNAME\Desktop\chatbot_project
cd C:\Users\$env:USERNAME\Desktop\chatbot_project

# 2) Create a Python virtual environment and activate it
python -m venv venv
.\venv\Scripts\Activate.ps1

# 3) Install required packages
python -m pip install --upgrade pip
python -m pip install flask nltk scikit-learn numpy

# 4) (Optional) download NLTK data to avoid runtime prompts
python - <<'PY'
import nltk
nltk.download('punkt')
nltk.download('wordnet')
PY

# 5) If you already have files (your project), skip to training. Otherwise create files as described below

# 6) Train the model (creates model.pkl & vectorizer.pkl)
python train.py

# 7) Run the Flask app
python app.py

# 8) Open browser to http://127.0.0.1:5000 to test the UI
```

## B. WSL / Linux / GitHub Codespaces (bash)
Open your distro or Codespace terminal and run:

```bash
# update + install packages (on WSL/Ubuntu; skip in Codespaces)
# sudo apt update && sudo apt install -y python3 python3-venv python3-pip git curl

# clone repo (if not already inside it) or cd into workspace
git clone https://github.com/<your-username>/Final-year-project-.git
cd Final-year-project-

# create + activate venv
python3 -m venv venv
source venv/bin/activate

# install deps
python3 -m pip install --upgrade pip
python3 -m pip install flask nltk scikit-learn numpy

# download NLTK data
python3 - <<'PY'
import nltk
nltk.download('punkt')
nltk.download('wordnet')
PY

# train and run
python3 train.py
python3 app.py

# open http://127.0.0.1:5000 on your browser
```

## How to create the required project files (copy-paste)
If the repository does not already contain the files, you can create them quickly using the commands below. Use the Bash block in Codespaces/WSL or the PowerShell variants in Windows.

Create a `.gitignore` (recommended) with these contents:

```text
venv/
__pycache__/
*.pyc
.DS_Store
model.pkl
vectorizer.pkl
chat_logs.txt
```

Create `intents.json` (college dataset — replace placeholders later)

```bash
cat > intents.json <<'JSON'
{ "intents": [
  {"tag":"greeting","patterns":["Hi","Hello","Hey"],"responses":["Hello! Welcome to Sri Krishna College of Arts and Science, Coimbatore."]},
  {"tag":"courses","patterns":["what courses do you offer","list of courses"],"responses":["We offer UG and PG programs including B.Sc, B.Com, BBA, BCA, BA, M.Sc and M.Com."]},
  {"tag":"admissions","patterns":["how to apply","admission process"],"responses":["Visit the Admissions page and fill the application form."]},
  {"tag":"fees","patterns":["fees","fee structure"],"responses":["Check the Fees page on the website for current fee structure."]},
  {"tag":"contact","patterns":["contact","phone number","email"],"responses":["Admissions Office: +91-XXXXXXXXXX. Email: admissions@example.edu"]},
  {"tag":"default_fallback","patterns":["*"],"responses":["I didn't understand that. Please rephrase or ask about admissions, courses, fees, contact details."]}
]}
JSON
```

Create `train.py` (copy-paste the entire script):

```bash
cat > train.py <<'PY'
import json
import nltk
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)

lem = WordNetLemmatizer()

def normalize(text):
    tokens = [t for t in word_tokenize(text.lower()) if t.isalnum()]
    return " ".join(lem.lemmatize(t) for t in tokens)

with open("intents.json", "r", encoding="utf-8-sig") as file:
    data = json.load(file)

X = []
y = []

for intent in data["intents"]:
    for pattern in intent["patterns"]:
        X.append(normalize(pattern))
        y.append(intent["tag"])

vectorizer = TfidfVectorizer(ngram_range=(1,2))
X_vectorized = vectorizer.fit_transform(X)

model = MultinomialNB()
model.fit(X_vectorized, y)

pickle.dump(model, open("model.pkl", "wb"))
pickle.dump(vectorizer, open("vectorizer.pkl", "wb"))

print("Model trained successfully")
PY
```

Create `app.py` (copy-paste):

```bash
cat > app.py <<'PY'
from flask import Flask, render_template, request, jsonify
import pickle
import json
import random
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import datetime

nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)

lem = WordNetLemmatizer()

def normalize(text):
    tokens = [t for t in word_tokenize(text.lower()) if t.isalnum()]
    return " ".join(lem.lemmatize(t) for t in tokens)

app = Flask(__name__)

model = pickle.load(open("model.pkl", "rb"))
vectorizer = pickle.load(open("vectorizer.pkl", "rb"))

with open("intents.json", "r", encoding="utf-8-sig") as f:
    intents = json.load(f)

CONFIDENCE_THRESHOLD = 0.50
LOGFILE = "chat_logs.txt"

def log_interaction(user_msg, pred_tag, confidence, reply):
    with open(LOGFILE, "a", encoding="utf-8") as f:
        f.write(f"[{datetime.datetime.now()}\t{user_msg}\t{pred_tag}\t{confidence:.3f}\t{reply}\n")

@app.route('/')
def home():
    return render_template('index.html')

@app.route('/get', methods=['POST'])
def chatbot():
    data = request.get_json(force=True)
    msg = data.get("message", "").strip()
    if not msg:
        return jsonify({"reply": "Please type a message."})

    vec = vectorizer.transform([normalize(msg)])
    try:
        probs = model.predict_proba(vec)[0]
        best_idx = probs.argmax()
        best_prob = probs[best_idx]
        tag = model.classes_[best_idx]
    except Exception:
        reply = "Sorry, I couldn't process that."
        log_interaction(msg, "error", 0.0, reply)
        return jsonify({"reply": reply})

    if best_prob < CONFIDENCE_THRESHOLD:
        reply = "I didn't understand that. Please rephrase or ask about courses, admissions, fees, contact, etc."
        log_interaction(msg, tag, best_prob, reply)
        return jsonify({"reply": reply})

    for intent in intents.get("intents", []):
        if intent.get("tag") == tag:
            reply = random.choice(intent.get("responses", ["Sorry."]))
            log_interaction(msg, tag, best_prob, reply)
            return jsonify({"reply": reply})

    reply = "I don't know the answer to that."
    log_interaction(msg, "unknown", best_prob, reply)
    return jsonify({"reply": reply})

if __name__ == '__main__':
    app.run(debug=True)
PY
```

Create the web UI files `templates/index.html` and `static/style.css` (copy the HTML/CSS from the repo or use the simple UI provided in earlier instructions).

## Training and running
- Train: `python train.py` → creates `model.pkl` and `vectorizer.pkl`.
- Run: `python app.py` → opens server at `http://127.0.0.1:5000`.

## Testing
Try these queries in the UI:
- "What courses do you offer?"
- "How much is the fee for B.Sc. Computer Science?"
- "How do I apply?"
- "Do you have hostel facilities?"

Inspect logs:

```bash
tail -n 100 chat_logs.txt
```

## Git & GitHub — push your project
1. Create a repo on GitHub (you already have one) and copy the URL.
2. Add remote (if not already):

```bash
git remote add origin https://github.com/<your-username>/Final-year-project-.git
git branch -M main
git add .
git commit -m "Initial project files"
git push -u origin main
```

If push asks for credentials, use a Personal Access Token (PAT) instead of password for HTTPS, or set up SSH keys.

## Polish before submission
- Replace placeholders in `intents.json` (phone numbers, emails, fee values) with real data.
- Expand `patterns` for key intents (30–100 examples each) to improve accuracy.
- Capture screenshots of the UI showing sample chats.
- Include `model.pkl` and `vectorizer.pkl` in your submission (or include `train.py` and instructions to regenerate them).

## Troubleshooting
- If `pip` is not recognized: use `python -m pip ...`.
- If you get JSON BOM errors, use `encoding="utf-8-sig"` when opening the file (already handled in `train.py`).
- If port 5000 is in use, change the port in `app.run(host='0.0.0.0', port=5001)`.

## Optional next steps (recommended for better project grade)
- Add multi-turn context and slot filling (follow-up questions like "Which program?").
- Add entity extraction (e.g., program names, fee queries) with regex or spaCy.
- Create a `Dockerfile` and deploy to Render/Railway for a live demo link.

## Support
If you want, I can now:
- generate a large dataset (200+ phrasing variants) — reply **large dataset**
- add multi-turn slot filling code — reply **multi-turn**
- create a `Dockerfile` + deployment instructions — reply **deploy**

Good luck with your final-year demo!
